from duckduckgo_search import DDGS
import bs4
from langchain_community.document_loaders import WebBaseLoader
from langchain.schema import Document
from utils import clean_web_content
import warnings
import re

warnings.filterwarnings("ignore", category=UserWarning)


def ddgs_results(query: str, max_results: int = 5):
    """
    Ricerca web con DDGS diretto, bypassa SSL e gestisce errori.
    """
    print(f"üîç DDGS: Ricerca per '{query}' (max {max_results} risultati)")
    
    try:
        # DDGS con SSL bypass
        with DDGS(
            verify=False,  # ‚Üê BYPASSA SSL
            timeout=20,
            headers={'User-Agent': 'Mozilla/5.0 (compatible; DDGSBot/1.0)'}
        ) as ddgs:
            
            results = list(ddgs.text(
                keywords=query,
                max_results=max_results,
                region='it-it',
                safesearch='moderate',
                timelimit='y'  # Risultati dell'ultimo anno
            ))
        
        print(f"   ‚úÖ {len(results)} risultati trovati")
        
        # Formatta per compatibilit√† con il resto del codice
        formatted = []
        for i, result in enumerate(results, 1):
            formatted.append(result.get('href', ''))
            print(f"   {i}. {result.get('title', '')[:50]}...")
        
        return formatted
        
    except Exception as e:
        print(f"‚ùå Errore DDGS: {e}")
        return []

def web_search_and_format(path: str):
    """
    Esegue una ricerca web, carica il contenuto e lo pulisce per un migliore processing.
    """
    print(f"üåê Caricamento contenuto da: {path}")
    
    try:
        # Strategia 1: Selettori CSS specifici per il contenuto principale
        content_selectors = [
            {"name": "article", "elements": ["article"]},
            {"name": "main-content", "elements": ["main", ".main", "#main"]},
            {"name": "content-areas", "elements": [".content", ".post-content", ".article-content", ".entry-content"]},
            {"name": "text-body", "elements": [".text", ".body", ".story-body", ".article-body"]},
        ]
        
        valid_docs = []
        
        for selector_group in content_selectors:
            if valid_docs:  # Se abbiamo gi√† trovato contenuto valido, fermati
                break
                
            try:
                loader = WebBaseLoader(
                    web_paths=(path,),
                    bs_kwargs=dict(
                        parse_only=bs4.SoupStrainer(selector_group["elements"])
                    )
                )
                
                docs = loader.load()
                print(f"   üîç Tentativo con selettori {selector_group['name']}: {len(docs)} documenti")
                
                for doc in docs:
                    # Pulisci il contenuto
                    cleaned_content = clean_web_content(doc.page_content)
                    
                    # Verifica che il contenuto pulito sia significativo
                    if len(cleaned_content.strip()) > 100:  # Almeno 100 caratteri dopo pulizia
                        # Aggiorna il documento con contenuto pulito
                        doc.page_content = cleaned_content
                        valid_docs.append(doc)
                        print(f"   ‚úÖ Contenuto valido trovato: {len(cleaned_content)} caratteri puliti")
                        break
                        
            except Exception as e:
                print(f"   ‚ö†Ô∏è Errore con selettori {selector_group['name']}: {e}")
                continue
        
        # Strategia 2: Se non abbiamo trovato nulla, prova senza filtri ma pulisci di pi√π
        if not valid_docs:
            print("   üîÑ Nessun contenuto valido trovato, provo senza filtri CSS...")
            try:
                loader = WebBaseLoader(web_paths=(path,))
                docs = loader.load()
                
                for doc in docs:
                    # Pulizia aggressiva per contenuto non filtrato
                    cleaned_content = clean_web_content(doc.page_content)
                    
                    # Rimozione aggiuntiva per contenuto non filtrato
                    lines = cleaned_content.split('\n')
                    content_lines = []
                    
                    for line in lines:
                        line = line.strip()
                        # Mantieni solo linee con contenuto sostanziale
                        if (len(line) > 30 and 
                            not re.match(r'^[A-Z\s]+$', line) and  # Non solo maiuscole
                            not re.match(r'^\d+$', line) and  # Non solo numeri
                            len(line.split()) > 3):  # Almeno 4 parole
                            content_lines.append(line)
                    
                    final_content = ' '.join(content_lines)
                    
                    if len(final_content.strip()) > 150:  # Standard pi√π alto per contenuto non filtrato
                        doc.page_content = final_content
                        valid_docs.append(doc)
                        print(f"   ‚úÖ Contenuto recuperato e pulito: {len(final_content)} caratteri")
                        break
                        
            except Exception as e:
                print(f"   ‚ùå Errore anche senza filtri: {e}")
        
        # Verifica finale e fallback
        if not valid_docs:
            print("   ‚ùå Impossibile estrarre contenuto significativo")
            return [Document(
                page_content=f"Contenuto non disponibile per {path}. Il sito web potrebbe non essere accessibile o non contenere testo leggibile.",
                metadata={"source": path, "error": "no_meaningful_content"}
            )]
        
        # Mostra preview del contenuto pulito
        for i, doc in enumerate(valid_docs):
            preview = doc.page_content[:200].replace('\n', ' ')
            print(f"   üìÑ Preview contenuto {i+1}: '{preview}...'")
        
        return valid_docs
        
    except Exception as e:
        print(f"‚ùå Errore generale nel caricamento di {path}: {e}")
        return [Document(
            page_content=f"Errore nel caricamento di {path}: {str(e)}",
            metadata={"source": path, "error": str(e)}
        )]